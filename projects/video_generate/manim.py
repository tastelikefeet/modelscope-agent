


def generate_manim_code(content,
                        content_type,
                        scene_number,
                        context_info=None,
                        surrounding_text='',
                        audio_duration=None,
                        main_theme='',
                        context_segments=None,
                        segment_index=0,
                        total_segments=None,
                        improvement_prompt=None,
                        existing_code=None):
    class_name = f'Scene{scene_number}'

    if not context_info:
        context_info = {
            'emphasis_words': [],
            'explanation_flow': [],
            'timing_cues': [],
            'emotional_tone': 'neutral',
            'complexity_level': 'medium'
        }

    print(f'ç”ŸæˆåŠ¨ç”»ä»£ç  - {content_type}: {class_name}')

    # æœ€ä¼˜å…ˆä½¿ç”¨å¹³è¡¡ç©ºé—´çº¦æŸç³»ç»Ÿï¼ˆæ£€æµ‹+LLMä¿®å¤ï¼‰
    if BALANCED_SPATIAL_AVAILABLE:
        print('ä½¿ç”¨å¹³è¡¡ç©ºé—´çº¦æŸç³»ç»Ÿï¼ˆå¢å¼ºæ£€æµ‹+å¤šè½®ä¿®å¤æ¨¡å¼ï¼‰...')

        # åˆ›å»ºå¹³è¡¡ç©ºé—´ç³»ç»Ÿ
        balanced_system = BalancedSpatialSystem()

        # ç”Ÿæˆå¹³è¡¡çš„æç¤ºè¯ï¼ˆé¿å…è¿‡åº¦å·¥ç¨‹åŒ–ï¼‰
        balanced_prompt = balanced_system.generate_balanced_prompt(
            content_type=content_type,
            content=content,
            class_name=class_name,
            audio_duration=audio_duration or 8.0)

        # è°ƒç”¨LLMç”Ÿæˆåˆå§‹ä»£ç 
        try:
            response = modai_model_request(
                balanced_prompt,
                model='Qwen/Qwen3-Coder-480B-A35B-Instruct',
                max_tokens=2000,
                temperature=0.7)

            # æå–ä»£ç 
            if '```python' in response:
                manim_code = response.split('```python')[1].split('```')[0]
            elif '```' in response:
                manim_code = response.split('```')[1].split('```')[0]
            else:
                manim_code = response

            # ğŸ” æ™ºèƒ½ä¿®å¤ç­–ç•¥é€‰æ‹©
            initial_analysis = balanced_system.analyze_and_score(manim_code)

            print('   åˆå§‹ä»£ç åˆ†æ:')
            print(f"   - å¸ƒå±€åˆ†æ•°: {initial_analysis['layout_score']}/100")
            print(f"   - å‘ç°é—®é¢˜: {initial_analysis['issue_count']}ä¸ª")

            # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦å†³å®šä¿®å¤ç­–ç•¥
            if initial_analysis['issue_count'] == 0:
                print('   [æˆåŠŸ] åˆå§‹ä»£ç å®Œç¾ï¼Œæ— éœ€ä¿®å¤')
                final_code = manim_code

            elif initial_analysis['issue_count'] <= 3 and initial_analysis[
                    'layout_score'] >= 80:
                print('é—®é¢˜è¾ƒå°‘ï¼Œä½¿ç”¨å•è½®ç²¾ç¡®ä¿®å¤')

                # å•è½®ä¿®å¤
                fix_prompt = balanced_system.generate_fix_prompt(
                    manim_code, initial_analysis['issues'])
                fix_request = f"""
{fix_prompt}

**åŸå§‹ä»£ç **:
```python
{manim_code}
```

è¯·ç²¾ç¡®ä¿®å¤æ£€æµ‹åˆ°çš„é—®é¢˜ï¼Œç¡®ä¿ä¿æŒåŠ¨ç”»æ•ˆæœçš„ä¸°å¯Œæ€§å’Œåˆ›æ„æ€§ã€‚
"""

                fix_response = modai_model_request(
                    fix_request,
                    model='Qwen/Qwen3-Coder-480B-A35B-Instruct',
                    max_tokens=2500,
                    temperature=0.3)

                # æå–ä¿®å¤åçš„ä»£ç 
                if '```python' in fix_response:
                    fixed_code = fix_response.split('```python')[1].split(
                        '```')[0]
                elif '```' in fix_response:
                    fixed_code = fix_response.split('```')[1].split('```')[0]
                else:
                    fixed_code = fix_response

                # éªŒè¯ä¿®å¤æ•ˆæœ
                final_analysis = balanced_system.analyze_and_score(fixed_code)

                if final_analysis['layout_score'] >= initial_analysis[
                        'layout_score']:
                    print(
                        f"   [æˆåŠŸ] å•è½®ä¿®å¤æˆåŠŸ: {initial_analysis['layout_score']} â†’ {final_analysis['layout_score']}"
                    )
                    final_code = fixed_code
                else:
                    print('   [è­¦å‘Š] å•è½®ä¿®å¤æ•ˆæœä¸ä½³ï¼Œä½¿ç”¨åŸå§‹ä»£ç ')
                    final_code = manim_code

            else:
                print('   ğŸ”„ é—®é¢˜è¾ƒå¤šï¼Œå¯ç”¨å¤šè½®ä¿®å¤æœºåˆ¶')

                # å¤šè½®ä¿®å¤
                fix_result = balanced_system.multi_round_fix(
                    manim_code, max_rounds=3)

                if fix_result['success']:
                    print('   [æˆåŠŸ] å¤šè½®ä¿®å¤æˆåŠŸ!')
                    print(f"   - æ€»æ”¹è¿›: +{fix_result['total_improvement']}åˆ†")
                    print(f"   - ä¿®å¤è½®æ•°: {fix_result['total_rounds']}")
                    final_code = fix_result['final_code']
                else:
                    print('   [è­¦å‘Š] å¤šè½®ä¿®å¤æœªå®Œå…¨æˆåŠŸï¼Œä½†å·²æœ‰æ”¹è¿›')
                    print(f"   - éƒ¨åˆ†æ”¹è¿›: +{fix_result['total_improvement']}åˆ†")
                    final_code = fix_result['final_code']

            # æœ€ç»ˆç®€å•ä¼˜åŒ–
            final_code = balanced_system.optimize_simple_code(final_code)

            return final_code

        except Exception as e:
            print(f'   å¹³è¡¡ç³»ç»Ÿå¤„ç†å¤±è´¥: {e}')
            # å›é€€åˆ°ç®€å•ä¼˜åŒ–
            try:
                basic_prompt = f'åˆ›å»º{content_type}ç±»å‹çš„ManimåŠ¨ç”»ï¼Œç±»å{class_name}ï¼Œå†…å®¹ï¼š{content}'
                response = modai_model_request(basic_prompt, max_tokens=1500)
                return clean_llm_code_output(response)
            except:  # noqa
                return create_simple_manim_scene(content_type, content,
                                                 class_name, '')

    # ä¼˜å…ˆä½¿ç”¨æ–°çš„ä¼˜åŒ–ç³»ç»Ÿ
    if OPTIMIZED_QUALITY_AVAILABLE:
        print('[å¯åŠ¨] ä½¿ç”¨ä¼˜åŒ–è´¨é‡æ§åˆ¶ç³»ç»Ÿ...')

        prompt_system = OptimizedManimPrompts()

        # å¦‚æœæœ‰ç°æœ‰ä»£ç ï¼Œå…ˆè¿›è¡Œåˆ†æ
        if existing_code:
            print('ğŸ“‹ åˆ†æç°æœ‰ä»£ç é—®é¢˜...')

        # æ„å»ºå†…å®¹æè¿°
        enhanced_content = content
        if improvement_prompt:
            enhanced_content = f'{content}\n\næ”¹è¿›è¦æ±‚ï¼š{improvement_prompt}'

        # ç”Ÿæˆä¼˜åŒ–çš„æç¤ºè¯
        generation_prompt = prompt_system.generate_creation_prompt(
            enhanced_content, content_type)

        # è°ƒç”¨LLMç”Ÿæˆä»£ç 
        enhanced_code = modai_model_request(
            prompt=generation_prompt, max_tokens=2048, temperature=0.1)

        if enhanced_code:
            # ä½¿ç”¨è´¨é‡æ§åˆ¶å™¨å¤„ç†ç”Ÿæˆçš„ä»£ç 
            controller = ManimQualityController(max_fix_attempts=2)
            result = controller.process_manim_code(enhanced_code, class_name,
                                                   enhanced_content)

            # è¾“å‡ºå¤„ç†æ—¥å¿—
            for log_entry in result.processing_log:
                print(log_entry)

            if result.success:
                print('[å®Œæˆ] ä»£ç ç”Ÿæˆå’Œè´¨é‡æ§åˆ¶å®Œæˆ')
                return result.final_code
            else:
                print('[è­¦å‘Š] è´¨é‡æ§åˆ¶éƒ¨åˆ†æˆåŠŸï¼Œä½¿ç”¨å½“å‰æœ€ä½³ç‰ˆæœ¬')
                return result.final_code

    # å›é€€åˆ°åŸæœ‰ç³»ç»Ÿ
    elif ENHANCED_PROMPTS_AVAILABLE:
        print('ä½¿ç”¨å¢å¼ºæç¤ºè¯ç³»ç»Ÿï¼ˆå›é€€æ¨¡å¼ï¼‰...')
        prompt_system = EnhancedManimPromptSystem()

        # å¦‚æœæœ‰æ”¹è¿›æç¤ºï¼Œå°†å…¶æ·»åŠ åˆ°å†…å®¹ä¸­
        enhanced_content = content
        if improvement_prompt:
            enhanced_content = f'{content}\n\n{improvement_prompt}'

        # ä¼ é€’ç°æœ‰ä»£ç ç”¨äºå¸ƒå±€åˆ†æ
        system_prompt, user_prompt = prompt_system.create_enhanced_prompt(
            content=enhanced_content,
            content_type=content_type,
            context_segments=context_segments,
            main_theme=main_theme,
            audio_duration=audio_duration,
            existing_code=existing_code  # æ–°å¢ï¼šä¼ é€’ç°æœ‰ä»£ç 
        )

        enhanced_code = modai_model_request(
            prompt=user_prompt,
            system_prompt=system_prompt,
            model='Qwen/Qwen3-Coder-480B-A35B-Instruct',
            max_tokens=2000,
            temperature=0.3,
            role='assistant')

        if enhanced_code:
            # æ¸…ç†LLMè¾“å‡ºçš„æ ¼å¼é—®é¢˜
            enhanced_code = clean_llm_code_output(enhanced_code)

            validation = prompt_system.validate_generated_code(
                enhanced_code, content_type)
            print(f"ä»£ç è´¨é‡å¾—åˆ†: {validation['validation_score']}/100")

            if validation['validation_score'] >= 70:
                print('å¢å¼ºæç¤ºè¯ç”ŸæˆæˆåŠŸ')
                return enhanced_code
            else:
                print('ä»£ç è´¨é‡è¾ƒä½ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•')
                for issue in validation['issues']:
                    print(f'- {issue}')

    if context_segments and total_segments and main_theme:
        print('å¯åŠ¨æ™ºèƒ½åˆ†æç³»ç»Ÿ...')
        optimization_data = optimize_animation(
            segment_content=content,
            segment_type=content_type,
            main_theme=main_theme,
            context_segments=context_segments,
            total_segments=total_segments,
            segment_index=segment_index)

        if 'error' not in optimization_data:
            optimized_script, enhanced_code = enhanced_script_and_animation_generator(
                original_content=content,
                content_type=content_type,
                main_theme=main_theme,
                optimization_data=optimization_data,
                class_name=class_name)

            if enhanced_code:
                print('æ™ºèƒ½ä¼˜åŒ–åŠ¨ç”»ç”Ÿæˆå®Œæˆ')
                return enhanced_code
            else:
                print('æ™ºèƒ½ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨å¢å¼ºç‰ˆç”Ÿæˆå™¨')
        else:
            print(f"æ™ºèƒ½åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å¢å¼ºç‰ˆç”Ÿæˆå™¨: {optimization_data['error']}")

    print('ä½¿ç”¨å¢å¼ºç‰ˆåŠ¨ç”»ç”Ÿæˆå™¨...')
    total_duration = audio_duration or 8.0
    return enhanced_generate_manim_code(content_type, content, class_name,
                                        surrounding_text, total_duration,
                                        context_info)